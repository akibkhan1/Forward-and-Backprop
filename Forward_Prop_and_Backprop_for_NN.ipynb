{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Forward Prop and Backprop for NN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNHIYsBcMVIzCfXdWgEhFh1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akibkhan1/Forward-and-Backprop/blob/main/Forward_Prop_and_Backprop_for_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njr30w3lrNEz"
      },
      "source": [
        "# Forward and Back Propagation implementation for Neural Networks\r\n",
        "\r\n",
        "Notebook Author:\r\n",
        "<br>\r\n",
        "Akib Mohammed Khan,\r\n",
        "<br>\r\n",
        "Islamic University of Technology (IUT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziHftvRfFfVD"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSYuOw7Uasv_"
      },
      "source": [
        "# Implement activation functions and derivative of ReLu function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp4SdGtmG0fS"
      },
      "source": [
        "def relu(x):\r\n",
        "   return np.maximum(0,x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBniZtNtHFm_"
      },
      "source": [
        "def sigmoid(x):\r\n",
        "  return 1/(1+np.exp(-X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIGz109_TKUX"
      },
      "source": [
        "def relu_prime(x):\r\n",
        "  res=[]\r\n",
        "  for i in range(np.size(x)):\r\n",
        "    if x[i]>0:\r\n",
        "      res.append(1)\r\n",
        "    else:\r\n",
        "      res.append(0)\r\n",
        "  return res"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp8wf2MRa2HY"
      },
      "source": [
        "# Initialize the weights and biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE5Qvb7FNafO"
      },
      "source": [
        "def initialise(n_x=2,n_h=4,n_y=1):\r\n",
        "  \"\"\"\r\n",
        "  n_x -- size of the input layer\r\n",
        "  n_h -- size of the hidden layer\r\n",
        "  n_y -- size of the output layer\r\n",
        "  \r\n",
        "  \"\"\"\r\n",
        "  W1 = np.random.randn(n_h, n_x) * 0.01\r\n",
        "  b1 = np.zeros(shape=(n_h, 1))\r\n",
        "  W2 = np.random.randn(n_y, n_h) * 0.01\r\n",
        "  b2 = np.zeros(shape=(n_y, 1))\r\n",
        "\r\n",
        "  params={\"W1\":W1,\r\n",
        "          \"b1\":b1,\r\n",
        "          \"W2\":W2,\r\n",
        "          \"b2\":b2}\r\n",
        "  return params"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld2WFMnMa9i-"
      },
      "source": [
        "# Implement forward propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QXtCOJvoohK"
      },
      "source": [
        "def forward_prop(X):\r\n",
        "  \"\"\"\r\n",
        "  X -- input data of shape (2, number of examples)\r\n",
        "  \"\"\"\r\n",
        "  W1 = params['W1']\r\n",
        "  b1 = params['b1']\r\n",
        "  W2 = params['W2']\r\n",
        "  b2 = params['b2']\r\n",
        "\r\n",
        "  Z1 = np.dot(W1, X) + b1\r\n",
        "  A1 = relu(Z1)\r\n",
        "  Z2 = np.dot(W2, A1) + b2\r\n",
        "  A2 = sigmoid(Z2)\r\n",
        "\r\n",
        "  temp = {\"Z1\": Z1,\r\n",
        "          \"A1\": A1,\r\n",
        "          \"Z2\": Z2,\r\n",
        "          \"A2\": A2}\r\n",
        "  \r\n",
        "  return A2, temp"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF3r6kgObZ83"
      },
      "source": [
        "# Implement backward propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB1xkJy7HeZI"
      },
      "source": [
        "def back_prop(X,Y,temp,params):\r\n",
        "  \"\"\"\r\n",
        "  params -- python dictionary containing our parameters \r\n",
        "  temp -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\r\n",
        "  X -- input data of shape (2, number of examples)\r\n",
        "  Y -- \"true\" labels vector of shape (1, number of examples)\r\n",
        "  \r\n",
        "  \"\"\"\r\n",
        "  A1 = temp['A1']\r\n",
        "  A2 = temp['A2']\r\n",
        "  Z1 = temp['Z1']\r\n",
        "\r\n",
        "  W1 = params['W1']\r\n",
        "  W2 = params['W2']\r\n",
        "\r\n",
        "  m = X.shape[1]\r\n",
        "\r\n",
        "  dZ2= A2 - Y\r\n",
        "  dW2 = (1 / m) * np.dot(dZ2, A1.T)\r\n",
        "  db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\r\n",
        "  dZ1 = np.multiply(np.dot(W2.T, dZ2), relu_prime(Z1))\r\n",
        "  dW1 = (1 / m) * np.dot(dZ1, X.T)\r\n",
        "  db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\r\n",
        "   \r\n",
        "  gradients = {\"dW1\": dW1,\r\n",
        "               \"db1\": db1,\r\n",
        "               \"dW2\": dW2,\r\n",
        "               \"db2\": db2}\r\n",
        "  \r\n",
        "  return gradients"
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}